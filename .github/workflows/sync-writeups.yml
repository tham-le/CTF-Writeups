name: üöÄ Sync CTF Writeups & Assets

on:
  push:
    paths:
      - 'sync-trigger.md'
  schedule:
    # Sync daily at 6 AM UTC to keep content fresh
    - cron: '0 6 * * *'
  workflow_dispatch:

jobs:
  sync-comprehensive:
    runs-on: ubuntu-latest
    
    steps:
    - name: üì• Checkout Portfolio Repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: üîß Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: üì¶ Install Dependencies
      run: |
        pip install PyYAML requests Pillow
        
    - name: üóÇÔ∏è Clone External CTF Repository
      run: |
        git clone https://github.com/tham-le/CTF-Writeups.git external-ctf-repo
        
    - name: üßπ Clean Previous Sync
      run: |
        rm -rf ctf_site/assets/writeups/*
        rm -rf ctf_site/assets/images/ctf/*
        mkdir -p ctf_site/assets/writeups
        mkdir -p ctf_site/assets/images/ctf
        
    - name: üöÄ Execute Comprehensive Sync
      run: |
        python3 << 'PYTHON_SCRIPT'
        import os
        import shutil
        import yaml
        import json
        import re
        from pathlib import Path
        from datetime import datetime
        from PIL import Image
        
        def optimize_image(source_path, target_path, max_size=(800, 600)):
            """Optimize images for web display"""
            try:
                with Image.open(source_path) as img:
                    # Convert to RGB if necessary
                    if img.mode in ('RGBA', 'LA', 'P'):
                        img = img.convert('RGB')
                    
                    # Resize if too large
                    img.thumbnail(max_size, Image.Resampling.LANCZOS)
                    
                    # Save with optimization
                    img.save(target_path, 'JPEG', quality=85, optimize=True)
                    print(f"‚úÖ Optimized image: {os.path.basename(target_path)}")
            except Exception as e:
                print(f"‚ö†Ô∏è Image optimization failed for {source_path}: {e}")
                # Fallback: just copy the original
                shutil.copy2(source_path, target_path)
        
        def process_markdown_content(content, source_path, target_dir):
            """Process markdown content and handle image references"""
            def replace_image_path(match):
                original_path = match.group(2)
                if original_path.startswith('./') or not original_path.startswith('http'):
                    # Handle local image
                    if original_path.startswith('./'):
                        source_img = os.path.join(os.path.dirname(source_path), original_path[2:])
                    else:
                        source_img = os.path.join(os.path.dirname(source_path), original_path)
                    
                    if os.path.exists(source_img):
                        img_name = os.path.basename(source_img)
                        # Create unique name to avoid conflicts
                        base_name, ext = os.path.splitext(img_name)
                        unique_name = f"{base_name}_{hash(source_path) % 10000}{ext}"
                        target_img = f"ctf_site/assets/images/ctf/{unique_name}"
                        
                        os.makedirs(os.path.dirname(target_img), exist_ok=True)
                        
                        # Optimize image if it's an image file
                        if ext.lower() in ['.png', '.jpg', '.jpeg']:
                            optimize_image(source_img, target_img)
                        else:
                            shutil.copy2(source_img, target_img)
                        
                        return f"![{match.group(1)}](../../images/ctf/{unique_name})"
                
                return match.group(0)
            
            # Replace image references
            content = re.sub(r'!\[([^\]]*)\]\(([^)]+)\)', replace_image_path, content)
            return content
        
        def extract_metadata_from_content(content, filepath):
            """Extract comprehensive metadata from markdown content"""
            metadata = {
                'title': os.path.basename(filepath).replace('.md', ''),
                'date': datetime.now().isoformat(),
                'category': 'misc',
                'difficulty': 'Medium',
                'points': 0,
                'tags': [],
                'author': 'Tham Le',
                'solved': True,
                'skills': [],
                'tools': [],
                'techniques': []
            }
            
            # Try YAML frontmatter first
            yaml_match = re.match(r'^---\n(.*?)\n---\n(.*)$', content, re.DOTALL)
            if yaml_match:
                try:
                    yaml_data = yaml.safe_load(yaml_match.group(1))
                    if yaml_data:
                        metadata.update(yaml_data)
                    content = yaml_match.group(2)
                except Exception as e:
                    print(f"‚ö†Ô∏è YAML parsing failed for {filepath}: {e}")
            
            # Extract title from first heading
            title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
            if title_match:
                metadata['title'] = title_match.group(1).strip()
            
            # Extract points/score
            points_match = re.search(r'(?:points?|score)[:\s]+(\d+)', content, re.IGNORECASE)
            if points_match:
                metadata['points'] = int(points_match.group(1))
            
            # Extract difficulty
            difficulty_match = re.search(r'(?:difficulty|level)[:\s]+(easy|medium|hard)', content, re.IGNORECASE)
            if difficulty_match:
                metadata['difficulty'] = difficulty_match.group(1).capitalize()
            
            # Extract tools and techniques from content
            tools_pattern = r'(?:using|with|via)\s+([A-Za-z0-9_-]+(?:\s+[A-Za-z0-9_-]+)?)'
            tools = re.findall(tools_pattern, content, re.IGNORECASE)
            metadata['tools'] = list(set(tools[:5]))  # Limit to 5 most relevant
            
            # Extract skills based on content analysis
            skill_keywords = {
                'web': ['XSS', 'SQL injection', 'CSRF', 'authentication', 'session', 'cookie', 'HTTP', 'web application'],
                'crypto': ['encryption', 'decryption', 'cipher', 'hash', 'RSA', 'AES', 'cryptography'],
                'forensics': ['memory dump', 'disk image', 'network capture', 'file analysis', 'steganography'],
                'pwn': ['buffer overflow', 'ROP', 'shellcode', 'binary exploitation', 'stack', 'heap'],
                'osint': ['social media', 'reconnaissance', 'information gathering', 'metadata', 'geolocation'],
                'rev': ['reverse engineering', 'disassembly', 'debugging', 'binary analysis', 'decompilation']
            }
            
            detected_skills = []
            content_lower = content.lower()
            for category, keywords in skill_keywords.items():
                for keyword in keywords:
                    if keyword.lower() in content_lower:
                        detected_skills.append(keyword)
            
            metadata['skills'] = list(set(detected_skills[:8]))  # Limit to 8 most relevant
            
            return metadata, content
        
        def discover_all_writeups(external_repo):
            """Discover all markdown files in the external repository"""
            writeups = []
            
            for root, dirs, files in os.walk(external_repo):
                # Skip hidden directories and git
                dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']
                
                for file in files:
                    if file.endswith('.md'):
                        filepath = os.path.join(root, file)
                        rel_path = os.path.relpath(filepath, external_repo)
                        
                        # Parse path structure: CTF/Category/Challenge/writeup.md
                        path_parts = rel_path.split(os.sep)
                        
                        if len(path_parts) >= 2:
                            ctf_event = path_parts[0]
                            
                            # Skip if it's a top-level README
                            if file == 'README.md' and len(path_parts) == 2:
                                continue
                            
                            # Determine category and challenge
                            if len(path_parts) == 3:  # CTF/Category/writeup.md
                                category = path_parts[1].lower()
                                challenge = file.replace('.md', '')
                            elif len(path_parts) == 4:  # CTF/Category/Challenge/writeup.md
                                category = path_parts[1].lower()
                                challenge = path_parts[2]
                            else:
                                category = 'misc'
                                challenge = file.replace('.md', '')
                            
                            # Normalize category names
                            category_mapping = {
                                'web_exploitation': 'web',
                                'web-exploitation': 'web',
                                'webexploitation': 'web',
                                'cryptography': 'crypto',
                                'binary_exploitation': 'pwn',
                                'binary-exploitation': 'pwn',
                                'binaryexploitation': 'pwn',
                                'reverse_engineering': 'rev',
                                'reverse-engineering': 'rev',
                                'reverseengineering': 'rev',
                                'open_source_intelligence': 'osint',
                                'open-source-intelligence': 'osint',
                                'opensourceintelligence': 'osint',
                                'miscellaneous': 'misc'
                            }
                            
                            category = category_mapping.get(category, category)
                            
                            writeups.append({
                                'filepath': filepath,
                                'ctf_event': ctf_event,
                                'category': category,
                                'challenge': challenge,
                                'relative_path': rel_path
                            })
            
            print(f"üîç Discovered {len(writeups)} writeup files")
            return writeups
        
        def sync_all_content():
            """Main sync function"""
            external_repo = "external-ctf-repo"
            target_base = "ctf_site/assets/writeups"
            
            # Discover all writeups
            discovered_writeups = discover_all_writeups(external_repo)
            
            # Statistics for recruiter appeal
            stats = {
                'total_writeups': len(discovered_writeups),
                'ctf_events': len(set(w['ctf_event'] for w in discovered_writeups)),
                'categories': {},
                'total_points': 0,
                'achievements': [],
                'skills_demonstrated': set(),
                'last_updated': datetime.now().isoformat(),
                'writeup_list': []
            }
            
            # Process each discovered writeup
            for writeup_info in discovered_writeups:
                try:
                    source_path = writeup_info['filepath']
                    ctf_event = writeup_info['ctf_event']
                    category = writeup_info['category']
                    challenge = writeup_info['challenge']
                    
                    # Create target directory structure
                    target_ctf_dir = os.path.join(target_base, ctf_event)
                    target_cat_dir = os.path.join(target_ctf_dir, category)
                    os.makedirs(target_cat_dir, exist_ok=True)
                    
                    # Read and process content
                    with open(source_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    # Extract metadata
                    metadata, processed_content = extract_metadata_from_content(content, source_path)
                    
                    # Process images and content
                    final_content = process_markdown_content(processed_content, source_path, target_cat_dir)
                    
                    # Add YAML frontmatter
                    frontmatter_data = {
                        'title': metadata['title'],
                        'ctf': ctf_event,
                        'category': category,
                        'challenge': challenge,
                        'difficulty': metadata['difficulty'],
                        'points': metadata['points'],
                        'solved': metadata['solved'],
                        'date': metadata['date'],
                        'author': metadata['author'],
                        'skills': metadata['skills'],
                        'tools': metadata['tools'],
                        'tags': metadata.get('tags', [])
                    }
                    
                    yaml_frontmatter = "---\n" + yaml.dump(frontmatter_data, default_flow_style=False) + "---\n\n"
                    
                    # Write processed file
                    target_file = os.path.join(target_cat_dir, f"{challenge}.md")
                    with open(target_file, 'w', encoding='utf-8') as f:
                        f.write(yaml_frontmatter + final_content)
                    
                    # Update statistics
                    if category not in stats['categories']:
                        stats['categories'][category] = 0
                    stats['categories'][category] += 1
                    stats['total_points'] += metadata['points']
                    stats['skills_demonstrated'].update(metadata['skills'])
                    
                    # Add to writeup list for frontend
                    stats['writeup_list'].append({
                        'title': metadata['title'],
                        'ctf': ctf_event,
                        'category': category,
                        'challenge': challenge,
                        'difficulty': metadata['difficulty'],
                        'points': metadata['points'],
                        'skills': metadata['skills'],
                        'path': f"{ctf_event}/{category}/{challenge}.md"
                    })
                    
                    print(f"‚úÖ Processed: {ctf_event}/{category}/{challenge}")
                    
                except Exception as e:
                    print(f"‚ùå Failed to process {writeup_info['filepath']}: {e}")
            
            # Copy achievement images
            for ctf_event in set(w['ctf_event'] for w in discovered_writeups):
                ctf_path = os.path.join(external_repo, ctf_event)
                
                # Look for achievement images
                for img_file in ['rank.png', 'rank.jpg', 'team_mvp.png', 'achievement.png', 'certificate.png']:
                    img_path = os.path.join(ctf_path, img_file)
                    if os.path.exists(img_path):
                        target_img = f"ctf_site/assets/images/ctf/{ctf_event}_{img_file}"
                        optimize_image(img_path, target_img)
                        stats['achievements'].append({
                            'ctf': ctf_event,
                            'image': f"../../images/ctf/{ctf_event}_{img_file}",
                            'type': img_file.replace('.png', '').replace('.jpg', '')
                        })
            
            # Convert sets to lists for JSON serialization
            stats['skills_demonstrated'] = list(stats['skills_demonstrated'])
            
            # Write statistics file
            with open('ctf_site/assets/stats.json', 'w') as f:
                json.dump(stats, f, indent=2)
            
            # Create master index
            create_master_index(stats, target_base)
            
            print(f"üéâ Sync completed successfully!")
            print(f"üìä Total: {stats['total_writeups']} writeups across {stats['ctf_events']} CTF events")
            print(f"üèÜ Categories: {list(stats['categories'].keys())}")
            print(f"üí° Skills: {len(stats['skills_demonstrated'])} unique skills demonstrated")
        
        def create_master_index(stats, target_base):
            """Create a master index file for easy navigation"""
            index_content = "# üèÜ Tham Le - CTF Writeups Portfolio\n\n"
            index_content += f"> Professional cybersecurity portfolio showcasing {stats['total_writeups']} writeups across {stats['ctf_events']} CTF competitions\n\n"
            index_content += "## üìä Portfolio Statistics\n\n"
            index_content += f"- **Total Writeups**: {stats['total_writeups']}\n"
            index_content += f"- **CTF Events**: {stats['ctf_events']}\n"
            index_content += f"- **Total Points**: {stats['total_points']}\n"
            index_content += f"- **Categories Mastered**: {len(stats['categories'])}\n"
            index_content += f"- **Skills Demonstrated**: {len(stats['skills_demonstrated'])}\n"
            index_content += f"- **Last Updated**: {datetime.now().strftime('%B %d, %Y')}\n\n"
            index_content += "## üéØ Skills Demonstrated\n\n"
            index_content += f"{', '.join(sorted(stats['skills_demonstrated']))}\n\n"
            index_content += "## üèÖ CTF Events Participated\n\n"
            
            # Group by CTF events
            ctf_groups = {}
            for writeup in stats['writeup_list']:
                ctf = writeup['ctf']
                if ctf not in ctf_groups:
                    ctf_groups[ctf] = []
                ctf_groups[ctf].append(writeup)
            
            for ctf_name, writeups in sorted(ctf_groups.items()):
                index_content += f"\n### üö© {ctf_name} ({len(writeups)} writeups)\n\n"
                
                # Group by category
                cat_groups = {}
                for writeup in writeups:
                    cat = writeup['category']
                    if cat not in cat_groups:
                        cat_groups[cat] = []
                    cat_groups[cat].append(writeup)
                
                for category, cat_writeups in sorted(cat_groups.items()):
                    index_content += f"**{category.upper()}**\n"
                    for writeup in sorted(cat_writeups, key=lambda x: x['challenge']):
                        skills_str = ', '.join(writeup['skills'][:3]) if writeup['skills'] else 'General'
                        index_content += f"- [{writeup['challenge']}](./{writeup['path']}) - {writeup['difficulty']} ({writeup['points']} pts) - *{skills_str}*\n"
                    index_content += "\n"
            
            # Write master index
            with open(os.path.join(target_base, 'index.md'), 'w', encoding='utf-8') as f:
                f.write(index_content)
            
            print("üìù Created master index file")
        
        # Execute the sync
        sync_all_content()
        PYTHON_SCRIPT
        
    - name: üìä Generate Portfolio Summary
      run: |
        echo "## üöÄ Portfolio Sync Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Sync Date**: $(date)" >> $GITHUB_STEP_SUMMARY
        if [ -f "ctf_site/assets/stats.json" ]; then
          echo "- **Total Writeups**: $(cat ctf_site/assets/stats.json | jq -r '.total_writeups')" >> $GITHUB_STEP_SUMMARY
          echo "- **CTF Events**: $(cat ctf_site/assets/stats.json | jq -r '.ctf_events')" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Points**: $(cat ctf_site/assets/stats.json | jq -r '.total_points')" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: üöÄ Deploy to Firebase
      run: |
        npm install -g firebase-tools
        firebase deploy --only hosting:ctf --token "${{ secrets.FIREBASE_TOKEN }}"
        
    - name: üìù Commit Changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "üöÄ Auto-sync CTF writeups and assets $(date)" || exit 0
        git push
